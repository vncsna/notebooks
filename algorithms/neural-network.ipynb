{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fix gradient descent with regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rv37KXchFEL4"
   },
   "source": [
    "#### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 1034,
     "status": "ok",
     "timestamp": 1605879332911,
     "user": {
      "displayName": "Vinicius Aguiar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIkJJn8xW1Jifzc3Yu1R42MS1Ter43iIMGz9IS=s64",
      "userId": "17869074096365757693"
     },
     "user_tz": 240
    },
    "id": "kTmzcxZ8FSgI"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from random import shuffle\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "  def __init__(self, ds, batch_size):\n",
    "    self.x = ds[0]\n",
    "    self.y = ds[1]\n",
    "    self.size = ds[0].shape[0]\n",
    "    self.batch_size = batch_size\n",
    "\n",
    "  def __iter__(self):\n",
    "    rnd = [*range(self.size)]\n",
    "    i = - self.batch_size\n",
    "    shuffle(rnd)\n",
    "    while i + self.batch_size < self.size:\n",
    "      i += self.batch_size\n",
    "      index = rnd[i:i + self.batch_size]\n",
    "      yield self.x[index], self.y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()\n",
    "\n",
    "x = digits.data / 16\n",
    "y = np.zeros((digits.target.size, digits.target.max() + 1))\n",
    "y[np.arange(digits.target.size), digits.target] = 1\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "  x, y, test_size=.1)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(\n",
    "  x_train, y_train, test_size=.1)\n",
    "\n",
    "train_dl = DataLoader((x_train, y_train), batch_size=4)\n",
    "valid_dl = DataLoader((x_valid, y_valid), batch_size=4)\n",
    "test_dl  = DataLoader((x_test , y_test),  batch_size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UfBgx2IcFI0L"
   },
   "source": [
    "#### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 2879,
     "status": "ok",
     "timestamp": 1605879334779,
     "user": {
      "displayName": "Vinicius Aguiar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIkJJn8xW1Jifzc3Yu1R42MS1Ter43iIMGz9IS=s64",
      "userId": "17869074096365757693"
     },
     "user_tz": 240
    },
    "id": "tz9rziLWh23d"
   },
   "outputs": [],
   "source": [
    "class Dense():\n",
    "  '''Dense layer.'''\n",
    "  def __init__(self, output_size):\n",
    "    self.input = None\n",
    "    self.weight = None\n",
    "    self.gradient = None\n",
    "    self.input_size = None\n",
    "    self.output_size = output_size\n",
    "\n",
    "  def init_weight(self, input_size):\n",
    "    '''Xavier et al. matrix initialization.'''\n",
    "    self.input_size = input_size + 1\n",
    "    shape = (self.input_size, self.output_size)\n",
    "    std = np.sqrt(2 / (self.input_size + self.output_size))\n",
    "    self.weight = np.random.normal(0, std, shape)\n",
    "\n",
    "  def __call__(self, x, phase):\n",
    "    self.input = x if phase == 'train' else None\n",
    "    bias = np.ones((x.shape[0], 1))\n",
    "    x = np.hstack((bias, x))\n",
    "    return np.dot(x, self.weight)\n",
    "  \n",
    "  def backpropagate(self, error, lambd=10**-3):\n",
    "    bias = np.mean(error, axis=0)\n",
    "    self.gradient  = np.dot(self.input.T, error) / self.input.shape[0]\n",
    "    self.gradient += self.weight[1:, :] * (lambd / self.input.shape[0])\n",
    "    self.gradient = np.vstack((bias, self.gradient))\n",
    "    return np.dot(error, self.weight[1:].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 2878,
     "status": "ok",
     "timestamp": 1605879334779,
     "user": {
      "displayName": "Vinicius Aguiar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIkJJn8xW1Jifzc3Yu1R42MS1Ter43iIMGz9IS=s64",
      "userId": "17869074096365757693"
     },
     "user_tz": 240
    },
    "id": "3brnT0sDr9dy"
   },
   "outputs": [],
   "source": [
    "class Sigmoid():\n",
    "  '''Sigmoid layer.'''\n",
    "  def __init__(self):\n",
    "    self.input = None\n",
    "\n",
    "  def __call__(self, x, phase):\n",
    "    self.input = x if phase == 'train' else None\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "  def backpropagate(self, error, lambd=10**-3):\n",
    "    z = self(self.input, 'valid')\n",
    "    return error * (z * (1 - z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 2877,
     "status": "ok",
     "timestamp": 1605879334780,
     "user": {
      "displayName": "Vinicius Aguiar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIkJJn8xW1Jifzc3Yu1R42MS1Ter43iIMGz9IS=s64",
      "userId": "17869074096365757693"
     },
     "user_tz": 240
    },
    "id": "J0dgJ6VZr94H"
   },
   "outputs": [],
   "source": [
    "class BinaryCrossEntropy():\n",
    "  '''Binary cross entropy cost function.'''\n",
    "  def __init__(self, lambd):\n",
    "    self.lambd = lambd\n",
    "  \n",
    "  def __call__(self, y, yh, layers=[]):\n",
    "    cost  = - y * np.log(yh + 10**-8)\n",
    "    cost += - (1 - y) * np.log(1 - yh + 10**-8)\n",
    "    cost = np.sum(cost) / y.shape[0]\n",
    "    reg = 0\n",
    "    for layer in layers:\n",
    "      if hasattr(layer, 'weight'):\n",
    "        reg += np.sum(layer.weight ** 2)\n",
    "    reg = (self.lambd * reg) / (2 * y.shape[0])\n",
    "    return cost + reg\n",
    "  \n",
    "  def accuracy(self, y, yh):\n",
    "    y  = np.argmax(y , axis=1)\n",
    "    yh = np.argmax(yh, axis=1)\n",
    "    return np.mean(y == yh)\n",
    "  \n",
    "  def backpropagate(self, y, yh):\n",
    "    return (yh - y) / (yh - yh * yh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 2875,
     "status": "ok",
     "timestamp": 1605879334780,
     "user": {
      "displayName": "Vinicius Aguiar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIkJJn8xW1Jifzc3Yu1R42MS1Ter43iIMGz9IS=s64",
      "userId": "17869074096365757693"
     },
     "user_tz": 240
    },
    "id": "FjTtzSh61Dsq"
   },
   "outputs": [],
   "source": [
    "class GradientDescent():\n",
    "  '''Gradient descent optimizer.'''\n",
    "  def __init__(self, alpha=3*10**-1):\n",
    "    self.alpha = alpha\n",
    "  \n",
    "  def step(self, layers):\n",
    "    for layer in layers:\n",
    "      if hasattr(layer, 'weight'):\n",
    "        layer.weight -= self.alpha * layer.gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 2875,
     "status": "ok",
     "timestamp": 1605879334781,
     "user": {
      "displayName": "Vinicius Aguiar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIkJJn8xW1Jifzc3Yu1R42MS1Ter43iIMGz9IS=s64",
      "userId": "17869074096365757693"
     },
     "user_tz": 240
    },
    "id": "TRVlYzZGsAOh"
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "  '''Neural network.'''\n",
    "  def __init__(self, input_size, layers):\n",
    "    self.layers = layers\n",
    "    for layer in self.layers:\n",
    "      if hasattr(layer, 'weight'):\n",
    "        layer.init_weight(input_size)\n",
    "        input_size = layer.output_size\n",
    "\n",
    "  def predict(self, x, phase='valid'):\n",
    "    for layer in self.layers:\n",
    "      x = layer(x, phase)\n",
    "    return x\n",
    "  \n",
    "  def backpropagate(self, error, lambd=10**-3):\n",
    "    for layer in self.layers[::-1]:\n",
    "      error = layer.backpropagate(error, lambd=lambd)\n",
    "  \n",
    "  def train(self, train_dl, cost_fun, optimizer, \n",
    "            valid_dl=None, epochs=20, verbose=True):\n",
    "    history = {}\n",
    "    acc, cost = self.test(train_dl, cost_fun, verbose=False)\n",
    "    history['train_acc']  = [acc]\n",
    "    history['train_cost'] = [cost]\n",
    "    acc, cost = self.test(valid_dl, cost_fun, verbose=False)\n",
    "    history['valid_acc']  = [acc]\n",
    "    history['valid_cost'] = [cost]\n",
    "    for epoch in range(epochs):\n",
    "      if verbose:\n",
    "        print(f'epoch {epoch + 1}')\n",
    "        print(f'lambda {cost_fun.lambd}')\n",
    "      for phase, dataloader in [('train', train_dl), ('valid', valid_dl)]:\n",
    "        if not dataloader:\n",
    "          continue\n",
    "        acc = 0\n",
    "        cost = 0\n",
    "        for x, y in dataloader:\n",
    "          yh = self.predict(x, phase=phase)\n",
    "          acc  += cost_fun.accuracy(y, yh) * dataloader.batch_size\n",
    "          cost += cost_fun(y, yh) * dataloader.batch_size       \n",
    "          if phase == 'train':\n",
    "            error = cost_fun.backpropagate(y, yh)\n",
    "            self.backpropagate(error, lambd=cost_fun.lambd)\n",
    "            optimizer.step(self.layers)\n",
    "        acc  = acc / dataloader.size\n",
    "        cost = cost / dataloader.size\n",
    "        history[f'{phase}_acc'].append(acc)\n",
    "        history[f'{phase}_cost'].append(cost)\n",
    "        if verbose:\n",
    "          print(f'{phase}_acc : {acc:.3f}')\n",
    "          print(f'{phase}_cost: {cost:.3f}')\n",
    "      if verbose:\n",
    "        clear_output(wait=True)\n",
    "    return history\n",
    "  \n",
    "  def test(self, dataloader, cost_fun, verbose=True):\n",
    "    acc = 0\n",
    "    cost = 0\n",
    "    for x, y in dataloader:\n",
    "      yh = self.predict(x)\n",
    "      acc  += cost_fun.accuracy(y, yh) * dataloader.batch_size\n",
    "      cost += cost_fun(y, yh) * dataloader.batch_size\n",
    "    acc = acc / dataloader.size\n",
    "    cost = cost / dataloader.size\n",
    "    if verbose:\n",
    "      print(f'test_acc: {acc:5.3f}')\n",
    "      print(f'test_cost: {cost:.3f}')\n",
    "    return acc, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 42\n",
      "lambda 0\n",
      "train_acc : 0.832\n",
      "train_cost: 2.049\n",
      "valid_acc : 0.864\n",
      "valid_cost: 2.046\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork(64, \n",
    "                   [Dense(25), \n",
    "                    Sigmoid(), \n",
    "                    Dense(10), \n",
    "                    Sigmoid()])\n",
    "\n",
    "history = nn.train(train_dl, \n",
    "                   BinaryCrossEntropy(lambd=0), \n",
    "                   GradientDescent(alpha=3*10**-3), \n",
    "                   valid_dl,\n",
    "                   epochs=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(range(43), history['train_acc'], s=4, label='Train')\n",
    "ax.scatter(range(43), history['valid_acc'], s=4, label='Validation')\n",
    "ax.legend()\n",
    "plt.xlim(0, 20)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc: 0.872\n",
      "test_cost: 2.002\n"
     ]
    }
   ],
   "source": [
    "nn.test(test_dl, BinaryCrossEntropy(lambd=0));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HS8BMNarWtR_"
   },
   "source": [
    "##### Gradient Checking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 8893,
     "status": "ok",
     "timestamp": 1605879340810,
     "user": {
      "displayName": "Vinicius Aguiar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIkJJn8xW1Jifzc3Yu1R42MS1Ter43iIMGz9IS=s64",
      "userId": "17869074096365757693"
     },
     "user_tz": 240
    },
    "id": "wn6dtMDB7cZV"
   },
   "outputs": [],
   "source": [
    "class NNDebugger():\n",
    "  def __init__(self, nn):\n",
    "    self.nn = nn\n",
    "\n",
    "  def get_weights(self):\n",
    "    weights = np.array([])\n",
    "    for layer in self.nn.layers:\n",
    "      if hasattr(layer, 'weight'):\n",
    "        new_weight = layer.weight.flatten()\n",
    "        weights = np.hstack((weights, new_weight))\n",
    "    return weights\n",
    "\n",
    "  def set_weights(self, weights):\n",
    "    for layer in self.nn.layers:\n",
    "      if hasattr(layer, 'weight'):\n",
    "        size = layer.weight.size\n",
    "        shape = layer.weight.shape\n",
    "        layer.weight = weights[:size].reshape(shape)\n",
    "        weights = weights[size:]\n",
    "\n",
    "  def get_grads(self):\n",
    "    grads = np.array([])\n",
    "    for layer in self.nn.layers:\n",
    "      if hasattr(layer, 'gradient') and np.all(layer.gradient):\n",
    "        new_grad = layer.gradient.flatten()\n",
    "        grads = np.hstack((grads, new_grad))\n",
    "    return grads\n",
    "  \n",
    "  def check_gradient(self, x, y, cost_fun, epsilon=10**-4):\n",
    "    yh = self.nn.predict(x, 'train')\n",
    "    error = cost_fun.backpropagate(y, yh)\n",
    "    self.nn.backpropagate(error, lambd=cost_fun.lambd)\n",
    "    weights = self.get_weights()\n",
    "    pertb = np.zeros_like(weights)\n",
    "    grads = self.get_grads()\n",
    "    num_grads = np.zeros_like(grads)\n",
    "    for i in range(weights.size):\n",
    "      pertb[i] = epsilon\n",
    "      self.set_weights(weights + pertb)\n",
    "      cost0 = cost_fun(y, self.nn.predict(x, 'valid'))\n",
    "      self.set_weights(weights - pertb)\n",
    "      cost1 = cost_fun(y, self.nn.predict(x, 'valid'))\n",
    "      num_grads[i] = (cost0 - cost1) / (2 * epsilon)\n",
    "      pertb[i] = 0\n",
    "    return grads, num_grads\n",
    "\n",
    "  def optimize(self, dataloader, cost_fun, filename):\n",
    "    def cost(weights):\n",
    "      print(weights[0])\n",
    "      clear_output(wait=False)\n",
    "      self.set_weights(weights)\n",
    "      return self.nn.test(dataloader, cost_fun, False)[1]\n",
    "    res = minimize(cost, \n",
    "                   self.get_weights(), \n",
    "                   method='CG',\n",
    "                   options={\n",
    "                       'maxiter': 20, \n",
    "                       'disp': True, \n",
    "                       'return_all': True})\n",
    "    np.savez_compressed(f'./{filename}', dict(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient checking with regularization 0\n",
      "Difference from numerical approximation:\n",
      "[ 3.44855096e-09  6.43872011e-10  4.03334342e-09  4.94660053e-10\n",
      " -2.74771293e-09  1.85070308e-09  2.94211931e-10  2.02623038e-09\n",
      "  2.12613453e-10 -1.51683433e-09  2.23054553e-09  4.03126891e-10\n",
      "  2.64913603e-09  3.22561672e-10 -1.76756687e-09  1.91378347e-09\n",
      "  1.23580596e-10  2.39745757e-09  1.02989099e-10 -1.55329413e-09\n",
      " -8.30653568e-09  5.63261479e-09  1.90351483e-08 -5.11282183e-09\n",
      "  3.21201882e-09  1.15260878e-08 -2.63814784e-09  1.95621648e-09\n",
      "  6.86175028e-09 -3.75076219e-09  2.35417258e-09  8.26498178e-09\n",
      " -1.81879860e-09  1.07876693e-09  3.16489224e-09 -3.36621698e-09\n",
      "  2.24619838e-09  7.11642775e-09]\n",
      "Difference nome from numerical approximation:\n",
      "3.0548903841144933e-08 \n",
      "\n",
      "Gradient checking with regularization 0.001\n",
      "Difference from numerical approximation:\n",
      "[ 1.76671381e-09 -3.61598302e-10  1.17622549e-09  8.69600994e-09\n",
      "  5.02175719e-09 -2.81236382e-04  1.39603165e-04  1.40804536e-04\n",
      " -9.31306640e-07  1.95137967e-04  7.71493772e-05 -1.34666278e-04\n",
      "  2.93920814e-04  8.22747051e-05 -2.45330769e-04 -3.46853086e-04\n",
      " -2.83083560e-04 -6.79167527e-05  1.41253401e-04 -4.97627379e-05\n",
      "  4.32175440e-09  8.17181062e-09  2.83796551e-08  6.54572118e-05\n",
      "  1.03830061e-04  5.11155252e-05 -7.77863854e-05  2.34146134e-04\n",
      " -7.35449643e-05  1.38916572e-04  4.58745277e-05  2.07630003e-05\n",
      "  4.20571580e-06  1.01545575e-04  3.76410429e-04  3.21246629e-04\n",
      "  1.10078030e-04  1.57722747e-04]\n",
      "Difference nome from numerical approximation:\n",
      "0.000979494154891761 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for lambd in [0, 10**-3]:\n",
    "  debugger = NNDebugger(NeuralNetwork(3, \n",
    "                                      [Dense(5), \n",
    "                                      Sigmoid(), \n",
    "                                      Dense(3), \n",
    "                                      Sigmoid()]))\n",
    "  grads, num_grads = debugger.check_gradient(np.random.rand(3, 3), \n",
    "                                            np.eye(3), \n",
    "                                            BinaryCrossEntropy(lambd=lambd))\n",
    "\n",
    "  print(f'Gradient checking with regularization {lambd}')\n",
    "  print('Difference from numerical approximation:')\n",
    "  print(grads - num_grads)\n",
    "  print('Difference nome from numerical approximation:')\n",
    "  print(np.linalg.norm(grads - num_grads), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eeqccRhvcW-l"
   },
   "source": [
    "#### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gvae4bpacYsl"
   },
   "source": [
    "[1] LeCun, Y., 1998. The MNIST database of handwritten digits. http://yann.lecun.com/exdb/mnist/.\n",
    "\n",
    "[2] Nielsen, M.A., 2015. Neural networks and deep learning (Vol. 2018). San Francisco, CA: Determination press.\n",
    "\n",
    "[3] Ng, A., 2017. Machine learning course. Coursera [online]. Available at: https://www.coursera.org/learn/machinelearning."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "projeto2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
