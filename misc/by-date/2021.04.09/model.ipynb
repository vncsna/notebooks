{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Previsão de Inadimplência"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leitura dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_parquet('data/train.parquet')\n",
    "test_df = pd.read_parquet('data/test.parquet')\n",
    "sample = pd.read_parquet('data/sub.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após seleção inicial de features realizamos a ordenação por data e normalização das colunas `divida_total`, `transacionado` e `pagamento_diario` por meio da divisão pelo `valor_emprestado`. Desta forma é possível comparar empresas cujo empréstimo e receitas tenham grandes diferenças. Em seguida extraímos os valores destas colunas para os primeiros 90 dias, preenchendo quando necessário. Este preenchimento foi realizado levando em conta um cenário pessimista, em que a dívida se mantém e não são realizados novos pagamentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort(df):\n",
    "    df = df.sort_values(by=['id', 'dias_pos_desembolso'])\n",
    "\n",
    "def normalize(df):\n",
    "    df['divida_total'] /= df['valor_emprestado']\n",
    "    df['transacionado'] /= df['valor_emprestado']\n",
    "    df['pagamento_diario'] /= df['valor_emprestado']\n",
    "\n",
    "def padding(x, n, mode):\n",
    "    x = np.array(x)\n",
    "    if x.size > n:\n",
    "        return x[:n]\n",
    "    return np.pad(x, (0, n - x.size), mode=mode)\n",
    "\n",
    "def format_features(df, n=90):\n",
    "    ids = df.id.unique()\n",
    "    x = np.zeros((len(ids), 3*n))\n",
    "    for i, idx in enumerate(ids):\n",
    "        dfi = df.query(f'id == {idx}')\n",
    "        debt = dfi.divida_total.values\n",
    "        debt = padding(debt, n, 'edge')\n",
    "        pay = dfi.pagamento_diario.values\n",
    "        pay = padding(pay, n, 'constant')\n",
    "        sales = dfi.transacionado.values\n",
    "        sales = padding(sales, n, 'constant')\n",
    "        x[i, :] = np.hstack((debt, pay, sales))\n",
    "    return x\n",
    "\n",
    "def format_targets(df):\n",
    "    targets = df.groupby('id').first()\n",
    "    targets = targets.y.values\n",
    "    return targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort(train_df)\n",
    "normalize(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = format_features(train_df)\n",
    "y = format_targets(train_df)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x, y, train_size=.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelagem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Escolhemos os modelos de regressão logística e máquina de vetores suporte com classes balanceadas após uma bateria de iterações."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_model(estimator, x, y):\n",
    "    y_pred = estimator.predict(x)\n",
    "    y_proba = estimator.predict_proba(x)[:, 1]\n",
    "    print(classification_report(y, y_pred, digits=4), end='')\n",
    "    print(f'\\tauroc\\t{roc_auc_score(y, y_proba):.4f}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelo Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9914    0.8050    0.8885     13264\n",
      "         1.0     0.1668    0.8478    0.2788       611\n",
      "\n",
      "    accuracy                         0.8068     13875\n",
      "   macro avg     0.5791    0.8264    0.5836     13875\n",
      "weighted avg     0.9551    0.8068    0.8616     13875\n",
      "\tauroc\t0.9066\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9894    0.7920    0.8798      3312\n",
      "         1.0     0.1577    0.8217    0.2646       157\n",
      "\n",
      "    accuracy                         0.7933      3469\n",
      "   macro avg     0.5736    0.8068    0.5722      3469\n",
      "weighted avg     0.9518    0.7933    0.8519      3469\n",
      "\tauroc\t0.8850\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
    "model = model.fit(x_train, y_train)\n",
    "\n",
    "assess_model(model, x_train, y_train)\n",
    "assess_model(model, x_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelo Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9915    0.7823    0.8746     13264\n",
      "         1.0     0.1531    0.8543    0.2597       611\n",
      "\n",
      "    accuracy                         0.7855     13875\n",
      "   macro avg     0.5723    0.8183    0.5671     13875\n",
      "weighted avg     0.9546    0.7855    0.8475     13875\n",
      "\tauroc\t0.9027\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9914    0.7699    0.8668      3312\n",
      "         1.0     0.1505    0.8599    0.2562       157\n",
      "\n",
      "    accuracy                         0.7740      3469\n",
      "   macro avg     0.5710    0.8149    0.5615      3469\n",
      "weighted avg     0.9534    0.7740    0.8391      3469\n",
      "\tauroc\t0.8906\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = SVC(class_weight='balanced', probability=True)\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "assess_model(model, x_train, y_train)\n",
    "assess_model(model, x_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussão"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No futuro gostaria de melhorar a modelagem explorando a aplicação de modelos de séries temporais, principalmente eliminando a restrição aos primeiros 90 dias. Talvez flexibilizando para atualizar a probabilidade semanalmente. Em geral não tive ideia boa o suficiente, que fizesse uma diferença satisfatória. Nem as tentativas de otimização de modelos mais complexos deram certo. *C'est la vie*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrega"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort(test_df)\n",
    "normalize(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = format_features(test_df)\n",
    "y_test_pred = model.predict(x_test)\n",
    "y_test_prob = model.predict_proba(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.DataFrame({\n",
    "    'id': test_df.id.unique(),\n",
    "    'y_pred': y_test_pred,\n",
    "    'y_prob': y_test_prob[:, 1],\n",
    "})\n",
    "sub_df.to_parquet('solution.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
