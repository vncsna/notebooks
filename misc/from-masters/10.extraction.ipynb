{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"b.extraction.ipynb","provenance":[],"collapsed_sections":["6jn7dBtaTSrC","TgruDSBljODw","ddYLk_jrA4vP","6Xf5nltMzRYT","KNEt4SRcqH_A","hKbylOvHqiFX","ckgYL0nCj8QD","WBOXJjPhSuKZ"],"mount_file_id":"1AARn2RRil3V5bL3SoT7DSf6bodaRojmw","authorship_tag":"ABX9TyMcKH6w1w2xL8wShmIj5by5"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"widgets":{"application/vnd.jupyter.widget-state+json":{"5fc371ae47a049ec851297f023144270":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_747fa195a98a4f149156b7b7cc588560","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_324a480d15c54886928a2f9cad576899","IPY_MODEL_3145cbd0221d4cff8e11b0661c4acc0e"]},"model_module_version":"1.5.0"},"747fa195a98a4f149156b7b7cc588560":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null},"model_module_version":"1.2.0"},"324a480d15c54886928a2f9cad576899":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_b662b992d0b14894b1f5e7cca7f80733","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":531456000,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":531456000,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_9258deb9f4c4472193547db65f5061bd"},"model_module_version":"1.5.0"},"3145cbd0221d4cff8e11b0661c4acc0e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_3caf66a43fbb4a7cba33b1985a70111e","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 507M/507M [00:13&lt;00:00, 39.9MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_401801e24442487cbad8a4e31da038f9"},"model_module_version":"1.5.0"},"b662b992d0b14894b1f5e7cca7f80733":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"},"model_module_version":"1.5.0"},"9258deb9f4c4472193547db65f5061bd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null},"model_module_version":"1.2.0"},"3caf66a43fbb4a7cba33b1985a70111e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"},"model_module_version":"1.5.0"},"401801e24442487cbad8a4e31da038f9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null},"model_module_version":"1.2.0"},"808156ea09d643ffa0673cd7097ba1da":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_4a162c81caae4512b9baf3ac640d3886","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_737d42ec6f524f5685d38b8867ad3652","IPY_MODEL_e00f5192517a41b8aa38a8197f0b5c7e"]},"model_module_version":"1.5.0"},"4a162c81caae4512b9baf3ac640d3886":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null},"model_module_version":"1.2.0"},"737d42ec6f524f5685d38b8867ad3652":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_57c722ecce1f4e54b15343bbfc8ab234","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":574673361,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":574673361,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ad398b756ddc4bcc9431cc2a800555ab"},"model_module_version":"1.5.0"},"e00f5192517a41b8aa38a8197f0b5c7e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_8810ad911f87491988c14fcd50b8e6ef","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 548M/548M [00:05&lt;00:00, 105MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_de01f02fce7d486dba40fd6d995bb075"},"model_module_version":"1.5.0"},"57c722ecce1f4e54b15343bbfc8ab234":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"},"model_module_version":"1.5.0"},"ad398b756ddc4bcc9431cc2a800555ab":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null},"model_module_version":"1.2.0"},"8810ad911f87491988c14fcd50b8e6ef":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"},"model_module_version":"1.5.0"},"de01f02fce7d486dba40fd6d995bb075":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null},"model_module_version":"1.2.0"}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"6jn7dBtaTSrC"},"source":["#### Setup"]},{"cell_type":"code","metadata":{"id":"6W8EXZHeiZWV"},"source":["from pathlib import Path\n","\n","import numpy as np\n","import scipy.sparse as sparse\n","import torch\n","import torch.nn as nn\n","from PIL import Image\n","from sklearn.decomposition import PCA\n","from torchvision import models, transforms\n","from IPython.display import clear_output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"swOruSJ21kl3","executionInfo":{"status":"ok","timestamp":1621601220812,"user_tz":240,"elapsed":1025,"user":{"displayName":"Vinicius Aguiar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIkJJn8xW1Jifzc3Yu1R42MS1Ter43iIMGz9IS=s64","userId":"17869074096365757693"}},"outputId":"c32a3d65-4107-43fd-aa45-5629baa91ff0"},"source":["%cd \"/content/drive/My Drive/archive/imecc/texture/data\""],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/archive/imecc/texture/data\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"TgruDSBljODw"},"source":["#### Utils"]},{"cell_type":"code","metadata":{"id":"WuxVUToKjRzm"},"source":["def get_files(dataset):\n","    \"\"\"Get files from a dataset folder.\"\"\"\n","    dataset = Path(dataset)\n","    files = [sorted(dataset.rglob(ext)) \n","           for ext in [\"*.png\", \"*.jpg\", \"*.bmp\", \"*.ppm\"]]\n","    files = [file for ext in files for file in ext]\n","    return files\n","\n","def zscore(arr):\n","    \"\"\"Measure an array z-score.\"\"\"\n","    return (arr - arr.mean(0)) / (arr.std(0) + 10 ** -8)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ddYLk_jrA4vP"},"source":["#### Feature Extraction: Backes"]},{"cell_type":"code","metadata":{"id":"Sd-3Igy9i6R8"},"source":["class Backes2013():\n","    \"\"\"Complex network image descriptor generator.\n","    From \"Texture analysis and classification:\n","    A complex network-based approach by Backes et al.\"\n","    \"\"\"\n","\n","    def gen_index(self, r, shape):\n","        \"\"\"Generate valid indices for graph modeling.\"\"\"\n","        x0 = []\n","        y0 = []\n","        k0 = []\n","        for i in range(shape[0]):\n","            for j in range(shape[1]):\n","                x0.append(i)\n","                y0.append(j)\n","                k0.append(i * shape[1] + j)\n","        x1 = []\n","        y1 = []\n","        k1 = []\n","        wd = []\n","        for i in range(-r, r + 1):\n","            for j in range(-r, r + 1):\n","                w = (i ** 2 + j ** 2) / (2 * r ** 2) # DIFF FROM Ribas\n","                if w > 0 and w <= .5:                # DIFF FROM Ribas\n","                    x1.append(i)\n","                    y1.append(j)\n","                    k1.append(i * shape[1] + j)\n","                    wd.append(w)\n","        len0 = len(x0)\n","        len1 = len(x1)\n","        x0 = np.repeat(x0, len1)\n","        y0 = np.repeat(y0, len1)\n","        k0 = np.repeat(k0, len1)\n","        x1 = x0 + np.tile(x1, len0)\n","        y1 = y0 + np.tile(y1, len0)\n","        k1 = k0 + np.tile(k1, len0)\n","        wd = np.tile(wd, len0)\n","        valid = len0 * len1 * [True]\n","        valid = np.logical_and(valid, x1 >= 0)\n","        valid = np.logical_and(valid, x1 < shape[0])\n","        valid = np.logical_and(valid, y1 >= 0)\n","        valid = np.logical_and(valid, y1 < shape[1])\n","        x0 = x0[valid]\n","        y0 = y0[valid]\n","        k0 = k0[valid]\n","        x1 = x1[valid]\n","        y1 = y1[valid]\n","        k1 = k1[valid]\n","        wd = wd[valid]\n","        return x0, y0, k0, x1, y1, k1, wd\n","  \n","    def extract(self, dataset, tag, r=3, tstart=.005, tstop=.515, tstep=.015):\n","        \"\"\"Extract complex network features \n","        from a graph by thresholding its edges.\"\"\" \n","        filepath = Path(f\"./{dataset}_{tag}.npz\")\n","        if not filepath.exists():\n","            files = get_files(dataset)\n","            im0 = Image.open(files[0]).convert(\"L\")\n","            nsteps = int((tstop - tstart) / tstep) + 1\n","            gr_shape = tuple(2 * [im0.size[0] * im0.size[1]])\n","            x0, y0, k0, x1, y1, k1, wd = self.gen_index(r, im0.size)\n","            X = {}\n","            X[\"contrast\"] = np.zeros((len(files), nsteps))\n","            X[\"energy\"] = np.zeros((len(files), nsteps))\n","            X[\"entropy\"] = np.zeros((len(files), nsteps))\n","            X[\"mean\"] = np.zeros((len(files), nsteps))\n","            for i, f in enumerate(files):\n","                img = Image.open(f)\n","                img = img.convert(\"L\")\n","                img = img.resize(im0.size)\n","                img = np.array(img, dtype=float)\n","                if img.shape != im0.size:\n","                    img = img.T\n","                w = wd + (img[x0, y0] - img[x1, y1]) / 510\n","                graph = sparse.csr_matrix((w, (k0, k1)), gr_shape)\n","                base_degseq = (graph > 0).sum(axis=0).A\n","                for j, t in enumerate(np.arange(tstart, tstop, tstep)):\n","                    degseq = base_degseq - (graph > t).sum(axis=0).A\n","                    deg, probs = np.unique(degseq, return_counts=True)\n","                    probs = probs / probs.sum()\n","                    X[\"contrast\"][i, j] = np.dot(deg * deg, probs)\n","                    X[\"energy\"][i, j] = np.dot(probs, probs)\n","                    X[\"entropy\"][i, j] = - np.dot(probs, np.log2(probs))\n","                    X[\"mean\"][i, j] = np.dot(deg, probs)\n","                print(f\"{dataset} loading {i + 1} / {len(files)}\")\n","                clear_output(wait=True)\n","            X[\"contrast\"] = zscore(X[\"contrast\"])\n","            X[\"energy\"] = zscore(X[\"energy\"])\n","            X[\"entropy\"] = zscore(X[\"entropy\"])\n","            X[\"mean\"] = zscore(X[\"mean\"])\n","            np.savez_compressed(filepath, **X)\n","        print(f\"{filepath.stem} extracted\")\n","        clear_output(wait=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZGCbN4rXfrN_"},"source":["# bc = Backes2013()\n","\n","# bc.extract(\"brodatz\", \"brodatz+bc.npz\")\n","# bc.extract(\"vistex\", \"vistex+bc.npz\")\n","# bc.extract(\"outex13i\", \"outex13i+bc.npz\")\n","# bc.extract(\"umd\", \"umd+bc.npz\")\n","# bc.extract(\"uiuc\", \"uiuc+bc.npz\")\n","# bc.extract(\"kthtips2b\", \"kthtips2b+bc.npz\")\n","# bc.extract(\"fmd\", \"fmd+bc.npz\")\n","# bc.extract(\"dtd\", \"dtd+bc.npz\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6Xf5nltMzRYT"},"source":["#### Feature Extraction: Ribas"]},{"cell_type":"code","metadata":{"id":"GQ3mWJ5pjtGb"},"source":["class Ribas2020:\n","    \"\"\"Neural and complex network image descriptor generator.\n","    From the paper Fusion of complex networks and\n","    randomized neural networks for texture analysis\n","    and classification by Ribas et al.\n","    \"\"\"\n","\n","    def gen_index(self, r, shape):\n","        \"\"\"Generate valid indices for graph modeling.\"\"\"\n","        x0 = []\n","        y0 = []\n","        k0 = []\n","        for i in range(shape[0]):\n","            for j in range(shape[1]):\n","                x0.append(i)\n","                y0.append(j)\n","                k0.append(i * shape[1] + j)\n","        x1 = []\n","        y1 = []\n","        k1 = []\n","        wd = []\n","        for i in range(-r, r + 1):\n","            for j in range(-r, r + 1):\n","                w = np.sqrt(i ** 2 + j ** 2)\n","                if w > 0 and w <= r:\n","                    x1.append(i)\n","                    y1.append(j)\n","                    k1.append(i * shape[1] + j)\n","                    wd.append(w)\n","        len0 = len(x0)\n","        len1 = len(x1)\n","        x0 = np.repeat(x0, len1)\n","        y0 = np.repeat(y0, len1)\n","        k0 = np.repeat(k0, len1)\n","        x1 = x0 + np.tile(x1, len0)\n","        y1 = y0 + np.tile(y1, len0)\n","        k1 = k0 + np.tile(k1, len0)\n","        wd = np.tile(wd, len0)\n","        valid = len0 * len1 * [True]\n","        valid = np.logical_and(valid, x1 >= 0)\n","        valid = np.logical_and(valid, x1 < shape[0])\n","        valid = np.logical_and(valid, y1 >= 0)\n","        valid = np.logical_and(valid, y1 < shape[1])\n","        x0 = x0[valid]\n","        y0 = y0[valid]\n","        k0 = k0[valid]\n","        x1 = x1[valid]\n","        y1 = y1[valid]\n","        k1 = k1[valid]\n","        wd = wd[valid]\n","        return x0, y0, k0, x1, y1, k1, wd\n","\n","    def gen_hidden_weight(self, Q, p):\n","        \"\"\"Generate a hidden weight matrix of neurons.\"\"\"\n","        E = Q * (p + 1)\n","        a = E + 2\n","        b = E + 3\n","        c = E ** 2\n","        V = [E + 1]\n","        for i in range(E - 1):\n","            V.append((a * V[-1] + b) % c)\n","        V = np.array(V)\n","        V = zscore(V)\n","        V = V.reshape(Q, p + 1)\n","        return V\n","\n","    def gen_output_weight(self, W, D, X):\n","        \"\"\"Generate output weight array of neurons.\"\"\"\n","        Ms = []\n","        for Xi in X:\n","            Z = np.tanh(np.dot(W, Xi))\n","            Z = np.vstack((- np.ones((1, Z.shape[1])), Z))\n","            M = np.dot(Z, Z.T) + (10 ** -3) * np.eye(Z.shape[0])\n","            M = np.dot(np.dot(D, Z.T), linalg.inv(M))\n","            Ms.append(M)\n","        return np.ravel(Ms)\n","\n","    def extract(self, dataset, tag, Q=[4, 14, 19], R=[4, 10]):\n","        \"\"\"Extract neural and complex network features from a graph.\"\"\"\n","        filepath = Path(f\"{dataset}_{tag}.npz\")\n","        if not filepath.exists():\n","            files = get_files(dataset)\n","            qr = [[q, r] for q in Q for r in R]\n","            im0 = Image.open(files[0])\n","            im0 = im0.convert(\"L\")\n","            img_shape = im0.size\n","            n_vertices = im0.size[0] * im0.size[1]\n","            gr_shape = (n_vertices, n_vertices)\n","            x0, y0, k0, x1, y1, k1, wd = self.gen_index(max(R), img_shape)\n","            W = {f\"Q{q}R{r}\": self.gen_hidden_weight(q, r) for q, r in qr}\n","            X = {f\"Q{q}R{r}\": np.zeros((len(files), 3 * q + 3)) for q, r in qr}\n","            for k, f in enumerate(files):\n","                img = Image.open(f)\n","                img = img.convert(\"L\")\n","                img = img.resize(img_shape)\n","                img = np.array(img, dtype=float)\n","                if img.shape != img_shape:\n","                    img = img.T\n","                D = img.ravel()\n","                has_edge = img[x0, y0] < img[x1, y1]\n","                wi = (img[x1, y1] - img[x0, y0]) / 255\n","                X0 = - np.ones((max(R) + 1, n_vertices))\n","                X1 = - np.ones((max(R) + 1, n_vertices))\n","                X2 = - np.ones((max(R) + 1, n_vertices))\n","                for r in range(1, max(R)):\n","                    i = np.logical_and(has_edge, wd <= r)\n","                    w = wi[i] + (wd[i] - 1) / (r - 1 + 10 ** -3)\n","                    graph = sparse.csr_matrix((w, (k0[i], k1[i])), gr_shape)\n","                    X0[r, :] = zscore((graph > 0).sum(axis=0).A.ravel())\n","                    X1[r, :] = zscore(graph.sum(axis=0).A.ravel())\n","                    X2[r, :] = zscore(graph.sum(axis=1).A.ravel())\n","                for q, r in qr:\n","                    Wqr = W[f\"Q{q}R{r}\"]\n","                    Xqr = [X0[:r + 1, :], X1[:r + 1, :], X2[:r + 1, :]]\n","                    X[f\"Q{q}R{r}\"][k, :] = self.gen_output_weight(Wqr, D, Xqr)\n","                print(f\"{dataset} loading {k + 1} / {len(files)}\")\n","                clear_output(wait=True)\n","            for key in X:\n","                X[key] = zscore(X[key])\n","            np.savez_compressed(f\"{dataset}_{tag}\", **X)\n","        print(f\"{filepath.stem} extracted\")\n","        clear_output(wait=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z-O5shUlA4vP"},"source":["# rb = Ribas2020()\n","\n","# rb.extract(\"brodatz\", \"brodatz+rb.npz\")\n","# rb.extract(\"vistex\", \"vistex+rb.npz\")\n","# rb.extract(\"outex13i\", \"outex13i+rb.npz\")\n","# rb.extract(\"umd\", \"umd+rb.npz\")\n","# rb.extract(\"uiuc\", \"uiuc+rb.npz\")\n","# rb.extract(\"kthtips2b\", \"kthtips2b+rb.npz\")\n","# rb.extract(\"fmd\", \"fmd+rb.npz\")\n","# rb.extract(\"dtd\", \"dtd+rb.npz\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KNEt4SRcqH_A"},"source":["#### Feature Extraction: CNN\n","(Remember that ResNet does not work)"]},{"cell_type":"code","metadata":{"id":"pXpc7qb5j32U"},"source":["class VGGCutModel(nn.Module):\n","    \"\"\"Truncated vgg model without the last 3 layers.\"\"\"\n","    def __init__(self, model):\n","        super(VGGCutModel, self).__init__()\n","        self.features = model.features\n","        self.avgpool = model.avgpool\n","        self.classifier = nn.Sequential(\n","            *list(model.classifier.children())[:-3])\n","  \n","    def forward(self, x):\n","        \"\"\"Forward pass of the truncated neural network model.\"\"\"\n","        with torch.no_grad():\n","            x = self.features(x)\n","            x = self.avgpool(x)\n","            x = self.classifier(x.view(-1))\n","            x = x.numpy().reshape(-1, )\n","        return x\n","\n","class ResNetCutModel():\n","    \"\"\"Truncated resnet model without the last layer.\"\"\"\n","    def __init__(self, model):\n","        super(ResNetCutModel, self).__init__()\n","        modules = list(model.children())[:-1]\n","        self.model = nn.Sequential(*modules)\n","        for p in self.model.parameters():\n","            p.requires_grad = False\n","\n","    def forward(self, x):\n","        \"\"\"Forward pass of the truncated neural network model.\"\"\"\n","        x = self.model.forward(x)\n","        x = x.numpy().reshape(-1, )\n","        return x\n","\n","class NNFeatureExtractor():\n","    \"\"\"Neural network feature extractor.\"\"\"\n","    def __init__(self, model, n_features):\n","        self.model = model\n","        self.n_features = n_features\n","        self.transform = transforms.Compose([\n","            transforms.Resize((224, 224)),\n","            transforms.ToTensor(),\n","            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                                 std=[0.229, 0.224, 0.225])\n","        ])\n","  \n","    def convert(self, img):\n","        \"\"\"Convert PIL image to an array of cnn features\"\"\"\n","        img = img.convert(\"RGB\")\n","        img = self.transform(img)\n","        img = img.view([1, 3, 224, 224])\n","        return self.model.forward(img)\n","        \n","    def extract(self, infolder, outfile):\n","        \"\"\"Extract CNN features from a image file\"\"\"\n","        outfile = Path(outfile)\n","        if not outfile.exists():\n","            files = get_files(infolder)\n","            x = np.zeros((len(files), self.n_features))\n","            for i, f in enumerate(files):\n","                img = Image.open(f)\n","                x[i, :] = self.convert(img)\n","                print(f\"{infolder} loading {i} / {len(files)}\")\n","                clear_output(wait=True)\n","            np.savez_compressed(outfile, x=x)\n","        print(f\"{outfile.stem} extracted\")\n","        clear_output(wait=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","referenced_widgets":["5fc371ae47a049ec851297f023144270","747fa195a98a4f149156b7b7cc588560","324a480d15c54886928a2f9cad576899","3145cbd0221d4cff8e11b0661c4acc0e","b662b992d0b14894b1f5e7cca7f80733","9258deb9f4c4472193547db65f5061bd","3caf66a43fbb4a7cba33b1985a70111e","401801e24442487cbad8a4e31da038f9"]},"id":"-GmSayDBV5fs","executionInfo":{"status":"ok","timestamp":1621601228820,"user_tz":240,"elapsed":7590,"user":{"displayName":"Vinicius Aguiar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIkJJn8xW1Jifzc3Yu1R42MS1Ter43iIMGz9IS=s64","userId":"17869074096365757693"}},"outputId":"1046b2bc-cf51-4844-cc33-288a9ab83e23"},"source":["model = VGGCutModel(models.vgg11(pretrained=True))\n","cnn = NNFeatureExtractor(model, 4096)\n","\n","cnn.extract(\"brodatz\", \"brodatz+vgg11.npz\")\n","cnn.extract(\"vistex\", \"vistex+vgg11.npz\")\n","cnn.extract(\"outex13i\", \"outex13i+vgg11.npz\")\n","cnn.extract(\"umd\", \"umd+vgg11.npz\")\n","cnn.extract(\"uiuc\", \"uiuc+vgg11.npz\")\n","cnn.extract(\"kthtips2b\", \"kthtips2b+vgg11.npz\")\n","cnn.extract(\"fmd\", \"fmd+vgg11.npz\")\n","cnn.extract(\"dtd\", \"dtd+vgg11.npz\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["dtd+vgg11 extracted\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","referenced_widgets":["808156ea09d643ffa0673cd7097ba1da","4a162c81caae4512b9baf3ac640d3886","737d42ec6f524f5685d38b8867ad3652","e00f5192517a41b8aa38a8197f0b5c7e","57c722ecce1f4e54b15343bbfc8ab234","ad398b756ddc4bcc9431cc2a800555ab","8810ad911f87491988c14fcd50b8e6ef","de01f02fce7d486dba40fd6d995bb075"]},"id":"_D-fbPIQDWys","executionInfo":{"status":"ok","timestamp":1621601235363,"user_tz":240,"elapsed":6546,"user":{"displayName":"Vinicius Aguiar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIkJJn8xW1Jifzc3Yu1R42MS1Ter43iIMGz9IS=s64","userId":"17869074096365757693"}},"outputId":"8571ecc7-b7b1-47ab-f5bd-67af5778e12a"},"source":["model = VGGCutModel(models.vgg19(pretrained=True))\n","cnn = NNFeatureExtractor(model, 4096)\n","\n","cnn.extract(\"brodatz\", \"brodatz+vgg19.npz\")\n","cnn.extract(\"vistex\", \"vistex+vgg19.npz\")\n","cnn.extract(\"outex13i\", \"outex13i+vgg19.npz\")\n","cnn.extract(\"umd\", \"umd+vgg19.npz\")\n","cnn.extract(\"uiuc\", \"uiuc+vgg19.npz\")\n","cnn.extract(\"kthtips2b\", \"kthtips2b+vgg19.npz\")\n","cnn.extract(\"fmd\", \"fmd+vgg19.npz\")\n","cnn.extract(\"dtd\", \"dtd+vgg19.npz\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["dtd+vgg19 extracted\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"hKbylOvHqiFX"},"source":["#### Feature Extraction: CNN + Visibility Graph"]},{"cell_type":"code","metadata":{"id":"k5k0de6Vkf0i"},"source":["class VisibilityGraph():\n","    \"\"\"Visibility graph generator.\"\"\"\n","    def __init__(self):\n","        self.sight = 0\n","        self.visibility = None\n","    \n","    def get_shape(self, files):\n","        \"\"\"Get image and its graph shape.\"\"\"\n","        im0 = Image.open(files[0])\n","        im0 = im0.convert(\"L\")\n","        img_shape = im0.size\n","        gr_shape = tuple(2 * [img_shape[0] * img_shape[1]])\n","        return img_shape, gr_shape\n","  \n","    def nvisibility(self, x, left, right, i, graph):\n","        \"\"\"Natural visibility graph algorithm.\"\"\"\n","        max_slope = float(\"-inf\")\n","        min_slope = float(\"+inf\")\n","        for j in np.arange(i + 1, right):\n","            slope = (x[j] - x[i]) / (j - i)\n","            if slope > self.sight:\n","                break\n","            if slope > max_slope:\n","                max_slope = slope\n","                graph[i, j] = 1\n","                graph[j, i] = 1\n","        for j in np.arange(i - 1, left, -1):\n","            slope = (x[i] - x[j]) / (i - j)\n","            if slope < - self.sight:\n","                break\n","            if slope < min_slope:\n","                min_slope = slope\n","                graph[i, j] = 1\n","                graph[j, i] = 1\n","\n","    def hvisibility(self, x, left, right, i, graph):\n","        \"\"\"Horizontal visibility graph algorithm.\"\"\"\n","        max_datum = float(\"-inf\")\n","        for j in np.arange(i + 1, right):\n","            if x[j] > max_datum:\n","                max_datum = x[j]\n","                graph[i, j] = 1\n","                graph[j, i] = 1\n","                max_datum = float(\"-inf\")\n","        for j in np.arange(i - 1, left, -1):\n","            if x[j] > max_datum:\n","                max_datum = x[j]\n","                graph[i, j] = 1\n","                graph[j, i] = 1\n","\n","    def wvisibility(self, x, left, right, i, graph):\n","        \"\"\"Weighted visibility graph algorithm.\"\"\"\n","        max_slope = float(\"-inf\")\n","        min_slope = float(\"+inf\")\n","        for j in np.arange(i + 1, right):\n","            slope = (x[j] - x[i]) / (j - i)\n","            if slope > self.sight:\n","                break\n","            if slope > max_slope:\n","                max_slope = slope\n","                graph[i, j] = np.arctan(slope)\n","                graph[j, i] = np.arctan(slope)\n","        for j in np.arange(i - 1, left, -1):\n","            slope = (x[i] - x[j]) / (i - j)\n","            if slope < - self.sight:\n","                break\n","            if slope < min_slope:\n","                min_slope = slope\n","                graph[i, j] = np.arctan(slope)\n","                graph[j, i] = np.arctan(slope)\n","\n","    def sort_and_conquer(self, x):\n","        \"\"\"Sort and conquer algorithm proposed by Ghosh et al.\"\"\"\n","        n = x.size\n","        sortd = np.argsort(x)[::-1]\n","        graph = sparse.lil_matrix((n, n))\n","        for i in np.arange(n):\n","            current = sortd[i]\n","            connected = graph.rows[current]\n","            left = -1\n","            right = n\n","            for j in connected:\n","                if j < current:\n","                    left = max(left, j)\n","                else:\n","                    right = min(right, j)\n","            self.visibility(x, left, right, current, graph)\n","        return sparse.csr_matrix(graph)\n","\n","    def extract(self, infile, mode, sight=0):\n","        \"\"\"Generate a (horizontal) visibility graph for each feature vector.\"\"\"\n","        \n","        # validation: if infile exists\n","        infile = Path(infile)\n","        if not infile.exists():\n","            print(f\"{infile} does not exist\")\n","            clear_output(wait=True)\n","            return\n","        \n","        # validation: if visibility is valid\n","        if mode == \"nvg\":\n","            outfile = f\"{infile.stem}+nvg.npz\"\n","            self.visibility = self.nvisibility\n","        elif mode == \"hvg\":\n","            outfile = f\"{infile.stem}+hvg.npz\"\n","            self.visibility = self.hvisibility\n","        elif mode == \"wvg\":\n","            outfile = f\"{infile.stem}+wvg{sight:05.2f}.npz\"\n","            self.visibility = self.wvisibility\n","        else:\n","            print(\"visibility does not exist\")\n","            clear_output(wait=True)\n","            return\n","        \n","        # validation: if outfile exists\n","        outfile = Path(outfile)\n","        if outfile.exists():\n","            print(f\"{outfile.stem} exists\")\n","            clear_output(wait=True)\n","            return\n","\n","        # validation: if sight greater or equal than zero\n","        if self.sight < 0:\n","            print(\"sight less than zero\")\n","            clear_output(wait=True)\n","            return\n","\n","        # extraction\n","        x = np.load(infile, allow_pickle=True)[\"x\"]\n","        degseq = np.zeros_like(x)\n","        for i in range(x.shape[0]):\n","            vg = self.sort_and_conquer(x[i, :])\n","            degseq[i, :] = vg.sum(axis=0).A\n","        np.savez_compressed(outfile, degseq=degseq)\n","        print(f\"{outfile.stem} extracted\")\n","        clear_output(wait=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LwbIBzByVODb","executionInfo":{"status":"ok","timestamp":1621601236390,"user_tz":240,"elapsed":587,"user":{"displayName":"Vinicius Aguiar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIkJJn8xW1Jifzc3Yu1R42MS1Ter43iIMGz9IS=s64","userId":"17869074096365757693"}},"outputId":"c1f436fe-d6d5-49b7-fc13-3c8d662267b0"},"source":["vg = VisibilityGraph()\n","\n","for dataset in [\"brodatz\", \"vistex\", \"outex13i\",\n","                \"umd\", \"uiuc\", \"kthtips2b\", \"fmd\", \"dtd\"]:\n","    for cnn in [\"vgg11\", \"vgg19\"]:\n","        infile = f\"{dataset}+{cnn}.npz\"\n","        vg.extract(infile, \"nvg\")\n","        vg.extract(infile, \"hvg\")\n","        vg.extract(infile, \"wvg\", 00.36)\n","        vg.extract(infile, \"wvg\", 00.84)\n","        vg.extract(infile, \"wvg\", 01.73)\n","        vg.extract(infile, \"wvg\", 05.67)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["dtd+vgg19+wvg05.67 exists\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ckgYL0nCj8QD"},"source":["#### Feature Extraction: Image Visibility Graph"]},{"cell_type":"code","metadata":{"id":"e7QPPV3Yj7ca"},"source":["class Iacovacci2018(VisibilityGraph):\n","    \"\"\"Image (horizontal) visibility graph generator as proposed by Iacovacci et al.\n","    From the paper visibility graphs for image processing by Iacovacci and Lacasa\n","    \"\"\"\n","    def __init__(self):\n","        super().__init__()\n","\n","    def gen_patch_profile(self, img, mode=\"n\"):\n","        \"\"\"Generate a patch profile z of an image (horizontal) visibility graph.\"\"\"\n","        Z = np.zeros(256)\n","        nvg_motiff = lambda a, b, c: c > 2 * b - a\n","        hvg_motiff = lambda a, b, c: b < a and b < c\n","        is_motiff = nvg_motiff if mode == \"n\" else hvg_motiff\n","        for i in range(img.shape[0] - 2):\n","            for j in range(img.shape[1] - 2):\n","                z  = 0\n","                z += 128 * is_motiff(img[i + 0, j], img[i + 0, j + 1], img[i + 0, j + 2])\n","                z += 64  * is_motiff(img[i + 1, j], img[i + 1, j + 1], img[i + 1, j + 2])\n","                z += 32  * is_motiff(img[i + 2, j], img[i + 2, j + 1], img[i + 2, j + 2])\n","                z += 16  * is_motiff(img[i, j + 0], img[i + 1, j + 0], img[i + 2, j + 0])\n","                z += 8   * is_motiff(img[i, j + 1], img[i + 1, j + 1], img[i + 2, j + 1])\n","                z += 4   * is_motiff(img[i, j + 2], img[i + 1, j + 2], img[i + 2, j + 2])\n","                z += 2   * is_motiff(img[i, j    ], img[i + 1, j + 1], img[i + 2, j + 2])\n","                z += 1   * is_motiff(img[i, j + 2], img[i + 1, j + 1], img[i + 2, j    ])\n","                Z[z] += 1\n","        return Z\n","\n","    def gen_degree_distribution(self, img, mode=\"n\"):\n","        \"\"\"Generate a degree distribution of a image (horizontal) visibility graph.\"\"\"\n","        row = []\n","        col = []\n","\n","        # set visibility mode\n","        if mode == \"n\":\n","            self.visibility = self.nvisibility\n","        elif mode == \"h\":\n","            self.visibility = self.hvisibility\n","\n","        # rows degree distribution\n","        j = np.arange(img.shape[1])\n","        for i in range(img.shape[0]):\n","            graph = self.sort_and_conquer(img[i, j])\n","            rowi, coli = graph.nonzero()\n","            ind = i * img.shape[1] + j\n","            rowi = [ind[k] for k in rowi]\n","            coli = [ind[k] for k in coli]\n","            row.extend(rowi)\n","            col.extend(coli)\n","\n","        # columns degree distribution\n","        i = np.arange(img.shape[0])\n","        for j in range(img.shape[1]):\n","            graph = self.sort_and_conquer(img[i, j])\n","            rowi, coli = graph.nonzero()\n","            ind = i * img.shape[1] + j\n","            rowi = [ind[k] for k in rowi]\n","            coli = [ind[k] for k in coli]\n","            row.extend(rowi)\n","            col.extend(coli)\n","\n","        # diagonals degree distribution\n","        for k in range(-img.shape[0] + 1, img.shape[1]):\n","            i = [max(0, -k)]\n","            j = [max(0, +k)]\n","            while i[-1] + 1 < img.shape[0] and j[-1] + 1 < img.shape[1]:\n","                i.append(i[-1] + 1)\n","                j.append(j[-1] + 1)\n","            i = np.array(i)\n","            j = np.array(j)\n","            graph = self.sort_and_conquer(img[i, j])\n","            rowi, coli = graph.nonzero()\n","            ind = i * img.shape[1] + j\n","            rowi = [ind[k] for k in rowi]\n","            coli = [ind[k] for k in coli]\n","            row.extend(rowi)\n","            col.extend(coli)\n","\n","        # degree distribution extraction\n","        w = np.ones(len(row))\n","        gr_shape = (img.size, img.size)\n","        graph = sparse.csr_matrix((w, (row, col)), gr_shape)\n","        deg, probs = np.unique(graph.sum(axis=0).A, return_counts=True)\n","        probs = probs / probs.sum()\n","        probs = probs.copy()\n","        probs.resize(30, refcheck=False)\n","        return probs\n","\n","    def extract(self, dataset, tag):\n","        \"\"\"Extract visibility graph features from specified images.\"\"\"\n","        files = get_files(dataset)\n","        img_shape, _ = self.get_shape(files)\n","        x = {}\n","        x[\"3patch_vg\"] = np.zeros((len(files), 256))\n","        x[\"3patch_hvg\"] = np.zeros((len(files), 256))\n","        x[\"degdist_vg\"] = np.zeros((len(files), 30))\n","        x[\"degdist_hvg\"] = np.zeros((len(files), 30))\n","        for k in range(len(files)):\n","            img = Image.open(files[k])\n","            img = img.convert(\"L\")\n","            img = np.array(img, dtype=float)\n","            img = img if img.shape == img_shape else img.T\n","            x[\"n\"] = k + 1\n","            x[\"3patch_vg\"][k, :] = self.gen_patch_profile(img)\n","            x[\"degdist_vg\"][k, :] = self.gen_degree_distribution(img)\n","            x[\"3patch_hvg\"][k, :] = self.gen_patch_profile(img, \"h\")\n","            x[\"degdist_hvg\"][k, :] = self.gen_degree_distribution(img, \"h\")\n","\n","        for key in x:\n","            x[key] = zscore(x[key])\n","        np.savez_compressed(f\"./{dataset}_{tag}\", **x)\n","        print(f\"{dataset} loading {k + 1} / {len(files)}\")\n","        clear_output(wait=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WBOXJjPhSuKZ"},"source":["#### ~~Feature Extraction: Blending Features~~"]},{"cell_type":"code","metadata":{"id":"zuN6YpvGlJi1"},"source":["class Blender:\n","    def reduce_dimension(self, x, threshold=.9):\n","        variance = 1\n","        n_components = 0\n","        pca = PCA().fit(x)\n","        while variance > threshold:\n","            n_components += 1\n","            variance = pca.explained_variance_ratio_[:-n_components].sum()\n","        x = pca.transform(x)[:, :n_components]\n","        return x\n","  \n","    # Falta terminar isso!!!\n","    def extract(self, old_tags, old_features, new_tag):\n","        datasets = [sorted(Path(\"./\").glob(tag)) for tag in old_tags]\n","        for dataset in zip(*datasets):\n","            filepath = dataset[0].stem.split(\"_\")[0]\n","            filepath = Path(f\"{filepath}_{new_tag}.npz\")\n","            if not filepath.exists():\n","                x = [np.load(datapath)[feature] \n","                    for i, datapath in enumerate(dataset) \n","                    for feature in old_features[i]]\n","                x = np.hstack(x)\n","                x = self.reduce_dimension(x)\n","                np.savez_compressed(filepath, x=x)\n","            clear_output()\n","            print(f\"{filepath.stem} extracted\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6SpRNvmgJJG5"},"source":["blender = Blender()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jhjUtIgbVqqy"},"source":["# Quais combinações eu analiso?"],"execution_count":null,"outputs":[]}]}